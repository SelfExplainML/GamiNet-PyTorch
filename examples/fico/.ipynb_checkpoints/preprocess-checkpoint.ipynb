{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T04:16:46.019095Z",
     "start_time": "2020-05-27T04:08:26.163560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column being fixed: 1\n",
      "Column being fixed: 8\n",
      "Column being fixed: 14\n",
      "Column being fixed: 17\n",
      "Column being fixed: 18\n",
      "Column being fixed: 19\n",
      "Column being fixed: 20\n",
      "Column being fixed: 21\n",
      "Column being fixed: 22\n"
     ]
    }
   ],
   "source": [
    "# --- Imports section --- \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "import copy\n",
    "\n",
    "class ModelError(Exception):\n",
    "\tpass\n",
    "\n",
    "class Data_Cleaner():\n",
    "\n",
    "\tdef __init__ (self, file_name, data = None):\n",
    "\t# --- Retrieves the data from CSV or array, as well as basic organisation ---\n",
    "\n",
    "\t\t# -- Get data from CSV or given array --\n",
    "\t\tif (data == None):\n",
    "\t\t\tself.data_set = pd.read_csv(file_name).values\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tself.data_set = data\n",
    "\n",
    "\t\t# -- Converting target to binary --\n",
    "\t\tnp.place(self.data_set, self.data_set == \"Bad\", 0)\n",
    "\t\tnp.place(self.data_set, self.data_set == \"Good\", 1)\n",
    "\n",
    "\t\t# -- Creating Model Variable -- \n",
    "\t\tself.model = None\n",
    "\n",
    "\t\t# -- Creating an Order Column --\n",
    "\t\torder = np.arange(self.data_set.shape[0])\n",
    "\t\torder = order.reshape((order.shape[0],1))\n",
    "\n",
    "\t\t# -- Scale and Split --\n",
    "\t\t# self.y = self.data_set[:,:1]\n",
    "\t\t# scaler = StandardScaler()\n",
    "\t\t# self.X = scaler.fit_transform(self.data_set[:,1:])\n",
    "\n",
    "\t\tself.y = self.data_set[:,:1]\n",
    "\t\tself.X = self.data_set[:,1:]\n",
    "\n",
    "\n",
    "\t\t# -- Needs to be retained for inserting new samples\n",
    "\t\t# self.mean = scaler.mean_\n",
    "\t\t# self.scale = scaler.scale_\n",
    "\n",
    "\t\t# -- Assiging general useful variables --\n",
    "\t\tself.num_samples , self.num_features = self.X.shape\n",
    "\n",
    "\t\t# -- Add the Order Column -- \n",
    "\t\tself.X = np.append(order,self.X,axis=1)\n",
    "\t\tself.y = np.append(order,self.y,axis=1)\n",
    "\n",
    "\tdef shift(self):\n",
    "\t# --- Perform the shift for the two categorical features --- \n",
    "\n",
    "\t\t# -- Shift is hardcoded based on requirements -- \n",
    "\t\tfirst_col = self.X[:,10]\n",
    "\t\tnp.place(first_col, first_col == 1, 100) # hold value\n",
    "\t\tnp.place(first_col, first_col == 6, 1)\n",
    "\t\tnp.place(first_col, first_col == 5, 1)\n",
    "\t\tnp.place(first_col, first_col == 4, 6)\n",
    "\t\tnp.place(first_col, first_col == 3, 5)\n",
    "\t\tnp.place(first_col, first_col == 2, 4)\n",
    "\t\tnp.place(first_col, first_col == 100, 3)\n",
    "\t\tnp.place(first_col, first_col == 0, 2)\n",
    "\t\tnp.place(first_col, first_col == 8, 0)\n",
    "\t\tnp.place(first_col, first_col == 9, 0)\n",
    "\n",
    "\t\tsecond_col= self.X[:,11]\n",
    "\t\tnp.place(second_col, second_col == 1, 0)\n",
    "\t\tnp.place(second_col, second_col == 9, 0)\n",
    "\t\tnp.place(second_col, second_col == 7, 1)\n",
    "\t\tnp.place(second_col, second_col == 8, 7)\n",
    "\n",
    "\t\tself.X[:,10] = first_col\n",
    "\t\tself.X[:,11] = second_col\n",
    "\n",
    "\tdef __scaled_row(self,row,scaler):\n",
    "\t# --- Returns the Row Scaled ---\n",
    "\t\tmean = scaler.mean_\n",
    "\t\tscale = scaler.scale_\n",
    "\t\tscld = []\n",
    "\t\tfor k in range(row.shape[0]):\n",
    "\t\t\tscld.append((row[k] - mean[k])/scale[k])\n",
    "\t\tscld = np.array(scld)\n",
    "\n",
    "\t\treturn scld\n",
    "\t        \n",
    "\tdef __masked_arr(self,orig_array, mask):\n",
    "\t# --- Returns XOR of Array and Mask --- \n",
    "\t\tmasked_array = []\n",
    "\n",
    "\t\tfor i in range(len(orig_array)):\n",
    "\t\t\trow = []\n",
    "\t\t\tfor j in range(len(orig_array[0])):\n",
    "\t\t\t\tif mask[j] != 0:\n",
    "\t\t\t\t\trow.append(orig_array[i][j])\n",
    "\t\t\tmasked_array.append(row)\n",
    "\n",
    "\t\tmasked_array = np.array(masked_array)\n",
    "\n",
    "\t\treturn masked_array\n",
    "\n",
    "\tdef __euc_distance(self,row1, row2):\n",
    "\t# --- Returns Euclidian Distance between Rows --- \n",
    "\t\tdist = 0\n",
    "\t\tfor i in range(len(row1)):\n",
    "\t\t\tt = (row1[i]-row2[i])**2\n",
    "\t\t\tdist += t\n",
    "\t\tdist = np.sqrt(dist)\n",
    "\t\treturn dist\n",
    "\n",
    "\tdef __predict_feature_weighted(self,row, good_data_masked, no_neighbours, orig_array, ft_idx):\n",
    "\t# --- Returns the single special value replaced by kNN imputation using weights---\n",
    "\n",
    "\t\tdistances = []\n",
    "\t\t# -- Loops through the good data with no special values -- \n",
    "\t\t\t# - Good data has the changing feature removed -\n",
    "\t\tfor i in range(len(good_data_masked)):\t\n",
    "\t\t\tdistances.append(self.__euc_distance(row, good_data_masked[i]))\n",
    "\n",
    "\t\tdistances = np.array(distances)\n",
    "\t\tmax_dist = np.max(distances)\n",
    "\t    \n",
    "\t\t# -- Sorts the first no_neigbours features --\n",
    "\t\tidx = np.argpartition(distances, no_neighbours)\n",
    "\n",
    "\t\tvalues = []\n",
    "\t\tmin_dists = []\n",
    "\t    \n",
    "\t\t# -- Retrieving values with which to replace -- \n",
    "\t\tfor i in range(no_neighbours):\n",
    "\t\t\tvalues.append(orig_array[idx[i]][ft_idx])\n",
    "\t\t\tmin_dists.append(distances[idx[i]])\n",
    "\n",
    "\t\tvalues = np.array(values) \n",
    "\t\tmin_dists = np.array(min_dists)\n",
    "\n",
    "\t\t# -- Assigning the weights -- \n",
    "\t\tweights = []\n",
    "\t\tfor i in min_dists:\n",
    "\t\t\tweights.append(1 - (i/max_dist))\n",
    "\t    \n",
    "\t    # -- Calculating final result -- \n",
    "\t\timputed_val = 0\n",
    "\t\tfor i in range(len(weights)):\n",
    "\t\t\timputed_val += weights[i] * values[i]\n",
    "\t        \n",
    "\t\treturn imputed_val    \n",
    "\n",
    "\tdef __predict_feature_mean(self,row, good_data_masked, no_neighbours, orig_array, ft_idx):\n",
    "\t# --- Returns the single special value replaced by kNN imputation using the mean ---\n",
    "\n",
    "\t\tdistances = []\n",
    "\t\t# -- Loops through the good data with no special values -- \n",
    "\t    \t# - Good data has the changing feature removed -\n",
    "\t\tfor i in range(len(good_data_masked)):\n",
    "\t\t\tdistances.append(self.__euc_distance(row,good_data_masked[i]))\n",
    "\t\tdistances = np.array(distances)\n",
    "\t    \n",
    "\t\t# -- Sorts the first no_neigbours features --\n",
    "\t\tidx = np.argpartition(distances, no_neighbours)\n",
    "\n",
    "\t\tvalues = []\n",
    "\t\tmin_dists = []\n",
    "\t    \n",
    "\t\t# -- Retrieving values with which to replace -- \n",
    "\t\tfor i in range(no_neighbours):\n",
    "\t\t\tvalues.append(orig_array[idx[i]][ft_idx])\n",
    "\t\t\tmin_dists.append(distances[idx[i]])\n",
    "\n",
    "\t\tvalues = np.array(values) \n",
    "\t\tmin_dists = np.array(min_dists)\n",
    "\t    \n",
    "\t\t# -- Calculating final result -- \n",
    "\t\timputed_val = 0\n",
    "\t\tfor i in range(len(values)):\n",
    "\t\t\timputed_val += values[i]\n",
    "\n",
    "\t\timputed_val = imputed_val/len(values)\n",
    "\n",
    "\t\treturn imputed_val\n",
    "\n",
    "\tdef __remove_row_with_vals(self, data, target, vals):\n",
    "\t# --- Returns the data/target without the rows that have any instance of vals list ---\n",
    "\t\tremoved_data = []\n",
    "\t\tremoved_target = []\n",
    "\n",
    "\t\trow_no = 0 \n",
    "\t\tfor row in data:\n",
    "\t\t\tfor col in row:\n",
    "\t\t\t\tif (col in vals):\n",
    "\t\t\t\t\tremoved_data.append(data[row_no])\n",
    "\t\t\t\t\tdata = np.delete(data, row_no, 0)\n",
    "\n",
    "\t\t\t\t\tremoved_target.append(target[row_no])\n",
    "\t\t\t\t\ttarget = np.delete(target, row_no, 0) \n",
    "\t\t\t\t\trow_no -= 1\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\trow_no += 1\n",
    "\n",
    "\t\tremoved_data = np.array(removed_data)\n",
    "\t\tremoved_target = np.array(removed_target)\n",
    "\n",
    "\t\treturn data, target, removed_data, removed_target\n",
    "\n",
    "\tdef __remove_col_with_vals(self, data, vals):\n",
    "\t# --- Returns the data without the coloumns that have the desired special values ---\n",
    "\t\tno_cols = data.shape[1]\n",
    "\t\tno_rows = data.shape[0]\n",
    "\t\trow = 0\n",
    "\t\twhile (no_rows > row):\n",
    "\t\t\tcol = 0\n",
    "\t\t\twhile (no_cols > col):\n",
    "\t\t\t\tif (data[row][col] in vals):\n",
    "\t\t\t\t\tdata = np.delete(data, col, 1)\n",
    "\t\t\t\t\tno_cols -= 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcol += 1\n",
    "\t\t\trow += 1     \n",
    "\t\treturn data\n",
    "\n",
    "\tdef __predict_values_lin_reg(self,X_tr,y_tr,X_test):\n",
    "\t# --- Uses linear regression to extrapolate values ---\n",
    "\t\tmodel = linear_model.LinearRegression()\n",
    "\t\tmodel.fit(X_tr, y_tr)\n",
    "\t\tpred = model.predict(X_test)\n",
    "\t\treturn pred\n",
    "\n",
    "\tdef __data_spliter(self,all_data,target_col,target_val):\n",
    "\t# --- Splits the data such to identify target col --- \n",
    "\t\ttarget_col += 1\n",
    "\n",
    "\t\ty = all_data[:,target_col:target_col+1]\n",
    "\t\tX = np.delete(all_data,target_col,1)\n",
    "\t    \n",
    "\t\t# -- Will hold the X for the y values that need to be predicted--\n",
    "\t\tX_target = np.zeros((1,X.shape[1]))\n",
    "\n",
    "\t\trow_no = 0 \n",
    "\t\t# -- Finds the rows with a target val -- \n",
    "\t\tfor val in y:\n",
    "\t\t\tif (val[0] == target_val):\n",
    "\t\t\t\tX_target = np.append(X_target,X[row_no:row_no+1,:],axis=0)\n",
    "\t\t\t\tX = np.delete(X, row_no, 0)\n",
    "\t\t\t\ty = np.delete(y, row_no, 0) \n",
    "\t\t\telse:\n",
    "\t\t\t\trow_no += 1\n",
    "\n",
    "\t\tX_target = np.delete(X_target,0,0)\n",
    "\t    \n",
    "\t\treturn X,y,X_target # Note that the order column is still attached\n",
    "\n",
    "\tdef __combine_parts_inorder(self,X,y,X_target,y_target,target_col):\n",
    "\t# --- Combines all the small parts into a single data matrix ---\n",
    "\t\ttarget_col += 1  # To account for the order column\n",
    "\n",
    "\t\ty_target = y_target.reshape((y_target.shape[0],1))\n",
    "\t\ty_full = np.append(y_target,y,axis=0)\n",
    "\t\tX_full = np.append(X_target,X,axis=0)\n",
    "\n",
    "\t\tdata = np.append(X_full[:,:target_col],y_full,axis=1)\n",
    "\t\tdata = np.append(data,X_full[:,target_col:],axis=1)\n",
    "\t\treturn data\n",
    "\n",
    "\tdef __average_each_feature(self,X):\n",
    "\t# --- Finds the mean values for each feature ---\n",
    "\n",
    "\t\tX_target = np.zeros((1,X.shape[1]))\n",
    "\t    \n",
    "\t\tfor i in range(X.shape[1]):\n",
    "\t\t\tcol = X[:,i]\n",
    "\t\t\tcol = np.mean(col,axis=0)\n",
    "\t\t\tX_target[:,i] = col\n",
    "\t        \n",
    "\t\treturn X_target\n",
    "\n",
    "\tdef __process_and_predict(self,all_data,target_col,target_val,exclude=None,model=\"linear\"):\n",
    "\t\t# -- Split data --\n",
    "\t\tX,y,X_target = self.__data_spliter(all_data,target_col,target_val)\n",
    "\t\t# -- Record order columns -- \n",
    "\n",
    "\t\torder_data = X[:,0:1]\n",
    "\t\torder_target = X_target[:,0:1]\n",
    "\t    \n",
    "\t    # -- Remove certain columns --\n",
    "\t\tif (exclude != None or exclude == []):\n",
    "\t\t\ty_tr = np.copy(y)\n",
    "\t\t\tX_tr = self.__remove_col_with_vals(X,exclude)\n",
    "\t\t\tX_tr = np.delete(X_tr,0,axis=1) # Removes the order column\n",
    "\t\t\tX_pred = self.__remove_col_with_vals(X_target,exclude) # The x used to predict\n",
    "\t\t\tX_pred = np.delete(X_pred,0,axis=1)\n",
    "\n",
    "\n",
    "\t\telse:\n",
    "\t\t\ty_tr = np.copy(y)\n",
    "\t\t\tX_tr = np.delete(X,0,axis=1) # Removes the order column\n",
    "\t\t\tX_pred = np.delete(X_target,0,axis=1)\n",
    "\n",
    "\n",
    "\t    # -- Run regression --\n",
    "\t\tif (model == \"linear\"):\n",
    "\t\t\ty_target = self.__predict_values_lin_reg(X_tr,y_tr,X_pred)\n",
    "\n",
    "\t\telif (model == \"polynomial\"):\n",
    "\t\t\tpass\n",
    "\n",
    "\t\telif (model == \"special\"):\n",
    "\t\t\tX_avg = self.__average_each_feature(X_pred)\n",
    "\t\t\tpred = self.__predict_values_lin_reg(X_tr,y_tr,X_avg)\n",
    "\t    \n",
    "\t\telse:\n",
    "\t\t\traise ModelError(\"Model currently not available\")\n",
    "\t        \n",
    "\t\tfinal_data = self.__combine_parts_inorder(X,y,X_target,y_target,target_col)\n",
    "\t\treturn final_data\n",
    "\t# --- Processes the data and uses linear regression to extrapolate --- \n",
    "\n",
    "\tdef remove_8(self, kNN, prediction_type):\n",
    "\t# --- Removes all the -8 values using kNN imputation ---\n",
    "\t\t# -- Remove the order column -- \n",
    "\t\torder = self.X[:,0]\n",
    "\t\torder = order.reshape((order.shape[0],1))\n",
    "\t\tself.X = np.delete(self.X, 0, axis = 1)\n",
    "\n",
    "\t\t# -- Removes all special values (-7,-8,-9) --\n",
    "\t\tX_good, hold1, hold2, hold3 = self.__remove_row_with_vals(self.X, self.y, [-7,-8,-9])\n",
    "\n",
    "\t\tscaler = StandardScaler()\n",
    "\t\tX_good_scaled = scaler.fit_transform(X_good)\n",
    "\n",
    "\t\t# -- Create a copy of the data matrix X to edit -- \n",
    "\t\tX_no_8 = np.copy(self.X)\n",
    "\n",
    "\t\tcols_with_8 = [1,8,14,17,18,19,20,21,22]\n",
    "\n",
    "\t\t# -- Fixing each -8 column -- \n",
    "\t\tfor fix_col in cols_with_8:\n",
    "\t\t\tprint(\"Column being fixed:\", str(fix_col))\n",
    "\t\t\t# -- Looping through all samples -- \n",
    "\t\t\tfor row in range(self.num_samples):\n",
    "\n",
    "\t\t\t\tif self.X[row][fix_col] == -8:\n",
    "\t\t\t\t\trow_to_comp = []\n",
    "\t\t\t\t\tmask = []\n",
    "\t\t\t\t\tscaled = self.__scaled_row(self.X[row],scaler)\n",
    "\n",
    "\t\t\t\t\t# -- Looping through each value --\n",
    "\t\t\t\t\tfor col in range(self.num_features):\n",
    "\t\t\t\t\t\tif self.X[row][col] >= 0:\n",
    "\t\t\t\t\t\t\tmask.append(1)\n",
    "\t\t\t\t\t\t\trow_to_comp.append(scaled[col])\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tmask.append(0)\n",
    "\n",
    "\t\t\t\t\trow_to_comp = np.array(row_to_comp)\n",
    "\t\t\t\t\tmask = np.array(mask)\n",
    "\t\t            \n",
    "\t\t\t\t\t# -- Getting the array of samples without special values in the good datasets-- \n",
    "\t\t\t\t\tX_good_masked = self.__masked_arr(X_good_scaled, mask)\n",
    "\n",
    "\t\t\t\t\tif (prediction_type == \"mean\"):\n",
    "\t\t\t\t\t\timputed = self.__predict_feature_mean(row_to_comp, X_good_masked, kNN, X_good_scaled, fix_col)\n",
    "\n",
    "\t\t\t\t\telif (prediction_type == \"weighted\"):\n",
    "\t\t\t\t\t\timputed = self.__predict_feature_weighted(row_to_comp, X_good_masked, kNN, X_good_scaled, fix_col)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tX_no_8[row][fix_col] = imputed*scaler.scale_[fix_col] + scaler.mean_[fix_col]\n",
    "\n",
    "\t\tself.X = X_no_8\n",
    "\n",
    "\t\t# -- Add back order column -- \n",
    "\t\tself.X = np.append(order,self.X,axis=1)\n",
    "\n",
    "\tdef remove_all_9(self):\n",
    "\t# --- Removes the columns with all -9 values -- \n",
    "\t\tself.rem_X = []\n",
    "\t\tself.rem_y = []\n",
    "\t\trow_no = 0 \n",
    "\t\tfor row in self.X:\n",
    "\t\t\tfor col_i in range(1,row.shape[0]):\n",
    "\t\t\t\tif (row[col_i] == -9):\n",
    "\t\t\t\t\tremove = True\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tremove = False\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tif remove:\n",
    "\t\t\t\tself.rem_X.append(self.X[row_no])\n",
    "\t\t\t\tself.X = np.delete(self.X, row_no, 0)\n",
    "\n",
    "\t\t\t\tself.rem_y.append(self.y[row_no])\n",
    "\t\t\t\tself.y = np.delete(self.y, row_no, 0)     \n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\trow_no += 1\n",
    "\n",
    "\t\tself.rem_X = np.array(self.rem_X)\n",
    "\t\tself.rem_y = np.array(self.rem_y)\n",
    "\n",
    "\tdef remove_9(self):\n",
    "\t# --- Removes the -9 values by using linear regression ---\n",
    "\t\tself.X = self.__process_and_predict(self.X,0,-9,[-7])\n",
    "\n",
    "\tdef remove_7_est(self):\n",
    "\t# --- Removes the -7 values by using an approximated value ---\n",
    "\t\tvalue_replace = 150\n",
    "\t\tnp.place(self.X, self.X == -7, value_replace)\n",
    "\n",
    "\tdef output_all_data(self):\n",
    "\t# --- Combines all the data into a single array in order and outputs it ---\n",
    "\t\tall_X = np.append(self.X,self.rem_X,axis=0)\n",
    "\t\tall_y = np.append(self.y,self.rem_y,axis=0)\n",
    "\n",
    "\t\tall_X = all_X[all_X[:,0].argsort()]\n",
    "\t\tall_y = all_y[all_y[:,0].argsort()]\n",
    "\n",
    "\t\tall_X = np.delete(all_X, 0, axis=1) \n",
    "\t\tall_y = np.delete(all_y, 0, axis=1) \n",
    "\t\t\n",
    "\t\tdata_output = np.append(all_y,all_X,axis=1)\n",
    "\n",
    "\t\treturn data_output\n",
    "\n",
    "\tdef output_to_CSV(self, filename):\n",
    "\t# --- Outputs the data to a CSV according to assigned filename --- \n",
    "\t\tdata_output = self.output_all_data()\n",
    "\n",
    "\t\tnp.savetxt(filename, data_output.astype(int), fmt='%i', delimiter=\",\")\n",
    "\n",
    "\tdef revert_to_original(self):\n",
    "\t# --- Allows to retrieve the original dataset ---\n",
    "\t\tself.__init__(\"pass\",self.data_set)\n",
    "\n",
    "testing123 = Data_Cleaner(\"./heloc_dataset_v1.csv\")\n",
    "testing123.shift()\n",
    "testing123.remove_8(5,\"mean\")\n",
    "testing123.remove_all_9()\n",
    "testing123.remove_9()\n",
    "testing123.remove_7_est()\n",
    "testing123.output_to_CSV(\"test_file1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:34:55.972490Z",
     "start_time": "2020-05-28T02:34:55.852496Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack([testing123.X, testing123.y])).to_csv(\"fico.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:36:57.680689Z",
     "start_time": "2020-05-28T02:36:57.624935Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"fico.csv\", index_col=[0, 1])\n",
    "x, y = data.iloc[:,0:].values, data.iloc[:,[-1]].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
